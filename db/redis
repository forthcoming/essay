lua script
Redis uses the same Lua interpreter to run all the commands. Also Redis guarantees that a script is executed in an atomic way:
no other script or Redis command will be executed while a script is being executed. This semantic is similar to the one of MULTI EXEC. 
From the point of view of all the other clients the effects of a script are either still not visible or already completed.
eval "return redis.call('get', KEYS[1])" 1 zgt          # 执行脚本,返回脚本的值,并注册脚本的sha值到redis
evalsha 4e6d8fc8bb01276962cce5371fa795a7763657ae 1 zgt  # 前提是sha已被注册
script exists 4e6d8fc8bb01276962cce5371fa795a7763657ae
script flush                                            # Flush all scripts from the script cache,redis重启or关闭也会触发该命令
script load "return redis.call('get', KEYS[1])"         # 注册脚本的sha值并返回sha,不执行脚本(sha1(b'lua script').hexdigest())

cluster
cluster info 
cluster nodes 
cluster slots  # returns details about which cluster slots map to which Redis instances
cluster keyslot key # 计算键key被放置在哪个槽上
cluster countkeysinslot <slot(num)>  # 计算槽上有多少个键值对
cluster getkeysinslot <slot(num)> <count> # 返回count个slot槽中的键

创建集群(客户端连接集群redis-cli需要带上-c,redis-cli -c -p port): 
1. redis-server 8001[8002|8003|8004|8005|8006|].conf,手动开启每个节点
2. redis-cli --cluster create 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006 --cluster-replicas 1
The option --cluster-replicas 1 means that we want a slave for every master created.(create a cluster with 3 masters and 3 slaves)
The other arguments are the list of addresses of the instances I want to use to create the new cluster.
集群中的主从是在创建集时分配,并不需要在每个节点的配置文件中设置
手动杀死集群中的某个master,其slave会自动被晋选为master

in practical terms, what do you get with Redis Cluster?
The ability to automatically split your dataset among multiple nodes.
The ability to continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster.

redis集群不支持事物
In redis-py-cluster, pipelining is all about trying to achieve greater network efficiency. 
Transaction support is disabled in redis-py-cluster. Use pipelines to avoid extra network round-trips, not to ensure atomicity.

Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot. 
The user can force multiple keys to be part of the same hash slot by using a concept called hash tags.
Hash tags are documented in the Redis Cluster specification, but the gist is that if there is a substring between {} brackets in a key, only what is inside the string is hashed, 
so for example this{foo}key and another{foo}key are guaranteed to be in the same hash slot, and can be used together in a command with multiple keys as arguments.
redis集群只支持db0,不支持mget,mset,multi,除非这些key落在同一个slot上,keys *只会返回该节点的数据(主从数据一样)
redis-py-cluster的StrictRedisCluster对keys,mget,mset,pipeline做了处理,使用时不需要再考虑key落在不同slot问题
其中pipeline原理是先根据for key in keys:crc16(key)%16384给keys分组,再批量执行,效率仍然比单条命令依次执行要高

什么时候整个集群不可用(cluster_state:fail)?
如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot映射[0-16383]不完整时进入fail状态
如果集群超过半数以上master挂掉,无论是否有slave,集群进入fail状态

Redis集群中内置了16384个哈希槽,当需要在Redis集群中放置一个key-value时,先对key使用crc16算法算出一个结果,然后把结果对16384求余数,
这样每个key都会对应一个编号在0-16383之间的哈希槽,redis会根据节点数量大致均等的将哈希槽映射到不同的节点(没使用一致性哈希)
投票过程是集群中所有master参与,如果半数以上master节点与master节点通信超时(cluster-node-timeout),认为当前master节点挂掉.

---------------------------------------------------------------------------------------------------------------------------------------

通用操作
del key1 key2 ... Keyn: 删除1个或多个键,不存在的key忽略掉,返回真正删除的key的数量
unlink key1 key2 ... Keyn: 删除1个或多个键,不存在的key忽略掉,return the number of keys that were unlinked,it is not blocking, while del is.
rename oldkey newkey: 给key赋一个新的key名, 如果newkey已存在 ,则 newkey的原值被覆盖 (类似的还有renamenx)
randomkey: 返回随机key
exists keyname: 判断key是否存在 ,返回 1/0
type keyname: 返回key的值类型 (eg:string,link,set,order set, hash)
ttl keyname: 返回key剩余的过期时间秒数(不过期的key返回-1,不存在的key返回-2)
persist keyname: 把指定key置为永久有效
expire keyname 整型值: 设置key的生命周期,以秒为单位(类似的还有pexpire,毫秒数,设置生命周期)
move keyname db: 将当前数据库的key移动到给定的数据库db当中,若与给定数据库key重名,或key不存在于当前数据库,则MOVE无任何效果
scan: 遍历所有键,类似的还有sscan,hscan,zscan,他们只遍历特定类型键里面的元素值
redis-cli -h 10.1.138.63 -n 1 --bigkeys: 分析数据库中的大key
monitor: streams back every command processed by the Redis server. It can help in understanding what is happening to the database
---------------------------------------------------------------------------------------------------------------------------------------

keys pattern:查找所有符合给定模式pattern的key
有3个通配符 *, ? ,[]
*:通配任意多个字符
?:通配单个字符
[]:通配括号内的某1个字符
127.0.0.1:6379> keys *
(empty list or set)
127.0.0.1:6379> mset one 1 two 2 three 3 four 4
OK
127.0.0.1:6379> keys o*
1) "one"
127.0.0.1:6379> keys *o
1) "two"
127.0.0.1:6379> keys ???
1) "one"
2) "two"
127.0.0.1:6379> keys on[dce]
1) "one"

---------------------------------------------------------------------------------------------------------------------------------------

string常见类型
set key value [ex 秒数]  [nx]/[xx] :  nx表示key不存在时执行操作,xx表示key存在时执行操作
get key
mget key1 key2...  :  类似的还有mset
setrange key offset value ：把字符串key的第offset个位置起替换成value，只覆盖value个长度
getrange key start stop ：获取字符串中 [start, stop]范围的值，左数从0开始,右数从-1开始
append key value
incr key:key值加1,并返回加1后的值，key必须是数字型字符串，对立操作是decr
getbit key offset : 获取值的二进制表示对应位上的值,offset从左从0编号,最大不能超过2^32-1,so key最大为512M
setbit key offset 0/1 : 设置offset对应二进制位上的值,返回该位上的旧值
bitcount key : 统计1的个数
bitop operation destkey key1 [key2 ...] : 对key1,key2..keyN作operation,并将结果保存到destkey上。
127.0.0.1:6379> setbit lower 2 1
(integer) 0
127.0.0.1:6379> get lower
" "
127.0.0.1:6379> set char Q
OK
127.0.0.1:6379> bitop or char char lower
(integer) 1
127.0.0.1:6379> get char
"q"

---------------------------------------------------------------------------------------------------------------------------------------

list常见命令(可以理解为链表)
llen key: 计算key元素个数
lindex key index :返回index索引上的值
lrem key count value :从key列表里移除前count次出现的值为value的元素(count>0从头往尾,count<0从尾往头,count=0移除所有)
lpush key value : 把值插入到list头部,值可以是多个
rpop key :  移除并返回存于key的最后一个元素
lrange key start stop: 返回链表中[start ,stop]中的元素,左数从0开始,右数从-1开始
ltrim key start stop: 使列表只存储[start,stop]范围内的数据,支持负索引

127.0.0.1:6379> lpush mylist a b 1
(integer) 3
127.0.0.1:6379> lrange mylist 0 -1
1) "1"
2) "b"
3) "a"
127.0.0.1:6379> rpop mylist
"a"

---------------------------------------------------------------------------------------------------------------------------------------

set常见命令(唯一性,无序性)
sadd key value1 value2:往集合key中增加元素
srem key value1 value2: 删除集合中集为value1 value2的元素,返回实际删除的元素个数
spop key:返回并删除集合中key中1个随机元素
srandmember key:返回集合key中随机的1个元素
smembers key:返回集中所有的元素
sismember key value:判断value是否在 key集合中
scard key:返回集合中元素的个数
smove source dest value:把source中的 value删除 ,并添加到 dest集合中
sinter  key1 key2 key3: 求出key1 key2 key3三个集合中的交集(公共部分) ,并返回
sinterstore dest key1 key2 key3:求出key1 key2 key3 三个集合中的交集 ,并赋给dest
suion key1 key2.. Keyn:求出key1 key2 keyn的并集并返回(类似的还有sunionstore)
sdiff key1 key2 key3:求出key1与key2 key3的差集,即key1-key2-key3

---------------------------------------------------------------------------------------------------------------------------------------

zset常见命令(有序集合,有序是指按照键score的大小顺序存放)
zadd key score1 value1 score2 value2:如果添加的成员已经存在于有序集合中,则会更新成员的score并更新到正确的排序位置
zcard key:返回元素个数
zrem key value1 value2: 删除集合中的元素
zrank key member: 返回member的排名(升续0名开始)
zrevrank key memeber:查询member的排名(降续0名开始)
zrange key start stop [withscores]:返回名次[start,stop]的元素,默认升续排列,Withscores是把score也打印出来(类似的还有zrevrange)
zrangebyscore  key min max [withscores] limit m n: 取score在 [min,max]内的元素
zcount key min max:返回[min,max] 区间内元素的数量
zremrangebyrank key start end:按排名删除元素,删除名次在 [start,end]之间的
zremrangebyscore key min max:按照socre来删除元素,删除 score在 [min,max]之间的

---------------------------------------------------------------------------------------------------------------------------------------

hash常见命令(可理解为字典)
hset key field value : 把key中filed域的值设为value,如果有field,则覆盖原field域的值
hmset key field1 value1 [field2 value2 field3 value3 ......fieldn valuen]
hdel key field : 删除key中field域
hlen key : 返回key中元素的数量
hexists key field : 判断key中有没有field域
hinrby float key field value : 把key中的field域的值增长浮点值value
hkeys key : 返回key中所有的field
hvals key : 返回key中所有的value
hgetall key : 返回key中所有得field-value

---------------------------------------------------------------------------------------------------------------------------------------

geohash
geoadd location 123.121 -34.12 'HK' 45.1 78.9 'Poland' 45.2 78.91 'Turkey'
# Data is stored into the key as a sorted set,先经度再纬度
# Latitude and Longitude bits are interleaved in order to form an unique 52 bit integer.We know that a sorted set double score can represent a 52 bit integer without losing precision.
# geohashEncodeWGS84(xy[0], xy[1], 26, &hash);
# GeoHashFix52Bits bits = geohashAlign52Bits(hash);

geopos location 'Poland'
geodist location 'Poland' 'Turkey' km
# 从zset中读出相应的double类型的score信息,
# GeoHashBits hash1={.bits = (uint64_t)score1,.step = 26};
# GeoHashBits hash2={.bits = (uint64_t)score2,.step = 26};
# geohashDecodeToLongLatWGS84(hash1, x1);
# geohashDecodeToLongLatWGS84(hash2, x2);
# return geohashGetDistance(x1,x2);
# geopos处理过程类似

geohash location 'HK'
# Returns an array with an 11 characters geohash representation of the position of the specified elements.
# The internal format we use for geocoding is a bit different than the standard, since we use as initial latitude range -85,85,
# while the normal geohashing algorithm uses -90,90.So we have to decode our position and re-encode using the standard ranges in order to output a valid geohash string.
# GeoHashBits hash = {.bits = (uint64_t)score, .step = 26};
# geohashDecodeToLongLatWGS84(hash, xy);
# GeoHashRange r[2]=[{.min = -180,.max = 180},{.min = -90,.max = 90}];
# GeoHashBits hash;
# geohashEncode(&r[0],&r[1],xy[0],xy[1],26,&hash);
# char *geoalphabet= "0123456789bcdefghjkmnpqrstuvwxyz";
# char buf[12];
# for (int i = 0; i < 11; i++) {
#     int idx = (hash.bits >> (52-((i+1)*5))) & 0x00011111;
#     buf[i] = geoalphabet[idx];
# }
# buf[11] = '\0';  # 此时的buf[10]一定等于'0'
# 注意：    
# int x=5;
# uint64_t y=5;
# x<<-2 => x<<30
# y>>-3 => x>>61

georadius location 45.1 78.88 4 km withdist  # 只返回location下的坐标
georadiusbymember location 'Poland' 3 km

---------------------------------------------------------------------------------------------------------------------------------------

hyperloglog
127.0.0.1:6379> pfadd hll a b c d a      
(integer) 1       # 1 if at least 1 HyperLogLog internal register was altered. 0 otherwise.
127.0.0.1:6379> pfcount hll
(integer) 4

---------------------------------------------------------------------------------------------------------------------------------------

安装redis到/usr/local/redis目录
$ wget http://download.redis.io/releases/redis-3.2.9.tar.gz
$ tar xzf redis-3.2.9.tar.gz
$ cd redis-3.2.9
$ make PREFIX=/opt/redis install #安装到指定目录中(没该目录则会自动创建)
$ mv redis.conf /opt/redis

ll ~/redis/bin
-rwxr-xr-x. 1 root root 2075842 Jan 31 01:10 redis-benchmark
-rwxr-xr-x. 1 root root   25173 Jan 31 01:10 redis-check-aof
-rwxr-xr-x. 1 root root   56020 Jan 31 01:10 redis-check-dump
-rwxr-xr-x. 1 root root 2205500 Jan 31 01:10 redis-cli
lrwxrwxrwx. 1 root root      12 Jan 31 01:10 redis-sentinel -> redis-server
-rwxr-xr-x. 1 root root 4358017 Jan 31 01:10 redis-server

redis-benchmark: redis性能测试工具
redis-check-aof: 检查aof日志的工具
redis-check-dump: 检查rbd日志的工具
redis-server /root/redis/redis.conf    # 指定启动redis时的配置文件

---------------------------------------------------------------------------------------------------------------------------------------

消息订阅
p = r.pubsub()
p.subscribe('my-first-channel')
p.psubscribe('my-*')
r.publish('my-first-channel', 'some data')
print(p.get_message())
print(p.get_message())
print(p.get_message())
print(p.get_message())
print(p.get_message())
r.publish('my-first-channel', 'some data')
print(p.get_message())
print(p.get_message())
print(p.get_message())
'''
With [un]subscribe messages, this value will be the number of channels and patterns the connection is currently subscribed to.
With [p]message messages, this value will be the actual published message.
{'type': 'subscribe', 'pattern': None, 'channel': b'my-first-channel', 'data': 1}
{'type': 'psubscribe', 'pattern': None, 'channel': b'my-*', 'data': 2}
{'type': 'message', 'pattern': None, 'channel': b'my-first-channel', 'data': b'some data'}
{'type': 'pmessage', 'pattern': b'my-*', 'channel': b'my-first-channel', 'data': b'some data'}
None
{'type': 'message', 'pattern': None, 'channel': b'my-first-channel', 'data': b'some data'}
{'type': 'pmessage', 'pattern': b'my-*', 'channel': b'my-first-channel', 'data': b'some data'}
None

while True:
    message = p.get_message()
    if message:
        # do something with the message
    time.sleep(0.001)  # be nice to the system :)
'''

---------------------------------------------------------------------------------------------------------------------------------------

事务
import redis
r = redis.Redis(host='localhost', port=6379, db=0)
r.flushdb()
pipe = r.pipeline(transaction=True) # The following SET commands are buffered
pipe.set('bing', 'baz')
pipe.set('foo', 'bar').get('bing')  # 所有缓冲到pipeline的命令返回pipeline对象本身,因此可以链式调用
pipe.execute()  # returning a list of responses, one for each command.  [True, True, b'baz']

'''
multi
It can never happen that a request issued by another client is served in the middle of the execution of a Redis transaction. 
This guarantees that the commands are executed as a single isolated operation.
Redis是单线程的服务,天生所有操作均具有原子性
事务状态是以一个事务为单位,执行事务队列中的所有命令:除非当前事务执行完毕,否则服务器不会中断事务,也不会执行其他客户端的其他命令
事物存在语法错误,则整个事务都不会执行
事务存在逻辑错误,比如set a 1,lpop a则会跳过该命令,执行剩下的命令,不支持回滚
exec
开始顺序执行各条命令,之后终止watch命令
discard
中止事务运行
watch
要求所有受监控的键在执行exec前都没有被修改时才会执行事务(相同客户端在事务内部修改这些键不影响事务的运行)
只能在客户端进入事务状态之前执行才有效

当前客户端的事务执行失败,程序需要做的就是不断重试这个操作,直到没有发生碰撞为止
这种形式的锁被称作乐观锁,它是一种非常强大的锁机制.因为大多数情况下不同的客户端会访问不同的键,碰撞的情况一般都很少,所以通常并不需要进行重试
'''

---------------------------------------------------------------------------------------------------------------------------------------

持久化(推荐两种方案同时使用)
快照(rdb)
每隔N分钟或者N次写操作后从内存dump数据形成rdb文件,压缩放在备份目录(导入导出速度快,容易出现丢失几分钟的数据,可以通过aof弥补)
快照配置选项
save 900 1           # 900内,有1 条写入,则产生快照
save 300 1000        # 如果300秒内有1000次写入,则产生快照(每300秒唤醒一次)
save 60 10000        # 如果60秒内有10000次写入,则产生快照(每60秒唤醒一次,从下往上看,这3个选项都屏蔽,则rdb禁用)

日志(aof)
# 注意在导出rdb过程中,aof如果停止同步,所有的操作缓存在内存的队列里,dump完成后统一操作
恢复时rdb比aof快,因为其是数据的内存映射,直接载入到内存,而aof是命令,需要逐条执行
当rdb跟aof同时开启时,则只加载aof里面的数据
主从关系中一般主开启aof,从开启一个rdb
当执行shutdown命令时会自动将内存中数据写进rdb(之前与aof不一致的数据会被覆盖掉)

---------------------------------------------------------------------------------------------------------------------------------------

key设计原则
1: 把表名转换为key前缀,如tag
2: 放置用于区分key的字段,对应mysql中的主键的列名,如userid
3: 放置主键值,如2,3,4...., a , b ,c
4: 存储的列名

create table book (bookid int,title char(20))engine myisam charset utf8;
insert into book values(5 , 'PHP圣经'),(6 , 'ruby实战'),(7 , 'mysql运维')(8, 'ruby服务端编程');
create table tags (tid int,bookid int,content char(20))engine myisam charset utf8;
insert into tags values(10 , 5 , 'PHP'),(11 , 5 , 'WEB'),(12 , 6 , 'WEB'),(13 , 6 , 'ruby'),(14 , 7 , 'database'),(15 , 8 , 'ruby'),(16 , 8 , 'server';

# 查询既有web标签又有PHP标签的书
select * from tags join tags as t on tags.bookid=t.bookid where tags.content='PHP' and t.content='WEB';

换成key-value存储
set book:bookid:5:title 'PHP 圣经'
set book:bookid:6:title 'ruby实战'
set book:bookid:7:title 'mysql运难'
set book:bookid:8:title 'ruby server'

sadd tag:PHP 5
sadd tag:WEB 5 6
sadd tag:database 7
sadd tag:ruby 6 8
sadd tag:SERVER 8

查:既有PHP又有WEB的书
sinter tag:PHP tag:WEB  # 查集合的交集

查:有PHP或有WEB标签的书
sunin tag:PHP tag:WEB

查:含有ruby不含WEB标签的书
sdiff tag:ruby tag:WEB # 求差集

注意:
在关系型数据中,除主键外,还有可能其他列也步骤查询
如上表中username也是极频繁查询的 ,往往这种列也是加了索引的
转换到k-v数据中,则也要相应的生成一条按照该列为主的key-value
set user:username:lisi:uid 9
我们可以根据username:lisi:uid,查出userid=9
再查user:9:password/email ...

---------------------------------------------------------------------------------------------------------------------------------------

sort
lpush num 12 33 -13 45 90
sort num asc  #默认升序,且按数值排序
sort num desc

lpush alp b c a test1 king ZendStudio
sort apl alpha desc
sort apl alpha asc limit 1 3

lpush uid 1
set user_1 admin
set user_level_1 9999
lpush uid 2
set user_2 king
set user_level_2 800
lpush uid 3
set user_3 queen
set user_level_3 600
127.0.0.1:6379> sort uid
1) "1"
2) "2"
3) "3"
127.0.0.1:6379> sort uid by user_level_* limit 1 2
1) "2"
2) "1"
127.0.0.1:6379> sort uid get user_*
1) "admin"
2) "king"
3) "queen"
127.0.0.1:6379> sort uid by user_level_* get user_*
1) "queen"
2) "king"
3) "admin"
127.0.0.1:6379> sort uid by user_level_* get user_* get user_level_*
1) "queen"
2) "600"
3) "king"
4) "800"
5) "admin"
6) "9999"

---------------------------------------------------------------------------------------------------------------------------------------

服务端命令
time  返回时间戳+微秒
dbsize 当前数据库未过期key的数量
bgrewriteaof 重写aof
bgsave 后台开启子进程dump数据
save 阻塞进程dump数据
lastsave
slaveof host port 做host port的从服务器(数据清空,复制新主内容)
slaveof no one 变成主服务器(原数据不丢失,一般用于主服失败后)
flushdb  清空当前数据库的所有数据
flushall 清空所有数据库数据
注: 
如果不小心运行了flushall,立即shutdown nosave(看作强制停止服务器的一个ABORT命令),关闭服务器然后手工编辑aof文件,去掉文件中的"flushall"相关行,然后开启服务器,就可以导入回原来数据
如果flushall之后,系统恰好bgrewriteaof了,那么aof就清空了,数据丢失
shutdown [save/nosave] 关闭服务器,保存数据,修改AOF(如果设置)
slowlog get N 获取慢查询日志
slowlog len 获取慢查询日志条数
slowlog reset 清空慢查询
info []  可以查看主从,内存/CPU使用,持久化,每个库使用情况
config get 选项(支持*通配)
config set 选项 值
config rewrite 把值写到配置文件
config restart 更新info命令的信息
debug object key #调试选项,看一个key的情况
debug segfault #模拟段错误,让服务器崩溃
object key (refcount|encoding|idletime)
monitor #打开控制台,观察命令(调试用)
client list #列出所有连接
client kill #杀死某个连接  CLIENT KILL 127.0.0.1:43501
client getname #获取连接的名称 默认nil
client setname "名称" #设置连接名称,便于调试

连接命令
auth 密码 #密码登陆(如果有密码)
ping #测试服务器是否可用
echo "some content" #测试服务器是否正常交互
select 0/1/2... #选择数据库,一个进程默认打开16个数据库,从0到15编号,可以从配置文件修改
quit #退出连接
