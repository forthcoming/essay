集群命令:
cluster info 
cluster slots  # returns details about which cluster slots map to which Redis instances
cluster nodes  # 返回信息跟cluster slots差不多,一行显示
cluster keyslot key # 计算键key被放置在哪个槽上
cluster countkeysinslot <slot(num)>  # 计算槽上有多少个键值对
cluster getkeysinslot <slot(num)> <count> # 返回count个slot槽中的键
cluster setslot <slot> importing <source-node-id>
将一个槽设置为importing状态,槽下的keys从指定源节点导入目标节点,该命令仅能在目标节点不是指定槽的所有者时生效
cluster setslot <slot> migrating <destination-node-id>
将一个槽设置为migrating状态,该命令的节点必须是该哈希槽的所有者,节点将会有如下操作: 
1. 如果处理的是存在的key,命令正常执行
2. 如果要处理的key不存在,接收命令的节点将发出一个重定向ASK,让客户端在destination-node重试该查询.在这种情况下客户端不应该将该哈希槽更新为节点映射.
3. 如果命令包含多个keys,如果都不存在处理方式同2;如果都存在,处理方式同1;
   如果只是部分存在,针对即将完成迁移至目标节点的keys按序返回TRYAGAIN错误,以便批量keys命令可以执行,The client can try the operation after some time, or report back the error.

# slave节点可以直接删除
# The first argument is just a random node in the cluster, the second argument is the ID of the node you want to remove.
# You can remove a master node in the same way as well, however in order to remove a master node it must be empty. 
# If the master is not empty you need to reshard data away from it to all the other master nodes before.
# An alternative to remove a master node is to perform a manual failover of it over one of its slaves and remove the node after it turned into a slave of the new master.
# Obviously this does not help when you want to reduce the actual number of masters in your cluster, in that case, a resharding is needed.
redis-cli --cluster del-node 127.0.0.1:8001 43c59528a792f3758c2df3b2c9fb18f86651904a
>>> Removing node 43c59528a792f3758c2df3b2c9fb18f86651904a from cluster 127.0.0.1:8001
>>> Sending CLUSTER FORGET messages to the cluster...
>>> SHUTDOWN the node.

redis-cli --cluster del-node 127.0.0.1:8006 9dd8ecfd28eae160d8c19d4184c4777f9bde49e8
>>> Removing node 9dd8ecfd28eae160d8c19d4184c4777f9bde49e8 from cluster 127.0.0.1:8006
[ERR] Node 127.0.0.1:8005 is not empty! Reshard data away and try again.

# 从节点不分配槽位
redis-cli --cluster check 127.0.0.1:8005
127.0.0.1:8005 (9dd8ecfd...) -> 13 keys | 5461 slots | 0 slaves.
127.0.0.1:8003 (1bf1f4e2...) -> 9 keys | 5461 slots | 1 slaves.
127.0.0.1:8002 (cba9eef5...) -> 21 keys | 5462 slots | 0 slaves.
[OK] 43 keys in 3 masters.
0.00 keys per slot on average.
>>> Performing Cluster Check (using node 127.0.0.1:8005)
M: 9dd8ecfd28eae160d8c19d4184c4777f9bde49e8 127.0.0.1:8005
   slots:[0-5460] (5461 slots) master
S: f72e454d7a592aef786ce318c4b73de6a0385a9d 127.0.0.1:8004
   slots: (0 slots) slave
   replicates 1bf1f4e2cefde5099452aec9dbc9386fea369777
M: 1bf1f4e2cefde5099452aec9dbc9386fea369777 127.0.0.1:8003
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
M: cba9eef5cf1b30a7b7dea7f2c8543ba2fe8b5e15 127.0.0.1:8002
   slots:[5461-10922] (5462 slots) master
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.

# 添加一个新节点8006到集群作为主节点,8006相关集群配置必须有且要运行起来,如果有cluster-config-file设置的文件,需要先删除
redis-cli --cluster add-node 127.0.0.1:8006 127.0.0.1:8002 
>>> Adding node 127.0.0.1:8006 to cluster 127.0.0.1:8002
>>> Performing Cluster Check (using node 127.0.0.1:8002)
M: cba9eef5cf1b30a7b7dea7f2c8543ba2fe8b5e15 127.0.0.1:8002
   slots:[5461-10922] (5462 slots) master
M: 1bf1f4e2cefde5099452aec9dbc9386fea369777 127.0.0.1:8003
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
S: f72e454d7a592aef786ce318c4b73de6a0385a9d 127.0.0.1:8004
   slots: (0 slots) slave
   replicates 1bf1f4e2cefde5099452aec9dbc9386fea369777
M: 9dd8ecfd28eae160d8c19d4184c4777f9bde49e8 127.0.0.1:8005
   slots:[0-5460] (5461 slots) master
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 127.0.0.1:8006 to make it join the cluster.
[OK] New node added correctly.

# 在主节点1bf1f4e2cefde5099452aec9dbc9386fea369777(8003)上添加一个新的8006从节点(可用来验证cluster-migration-barrier配置)
redis-cli --cluster add-node 127.0.0.1:8006 127.0.0.1:8002 --cluster-slave --cluster-master-id 1bf1f4e2cefde5099452aec9dbc9386fea369777 
>>> Adding node 127.0.0.1:8006 to cluster 127.0.0.1:8002
>>> Performing Cluster Check (using node 127.0.0.1:8002)
M: cba9eef5cf1b30a7b7dea7f2c8543ba2fe8b5e15 127.0.0.1:8002
   slots:[5461-10922] (5462 slots) master
M: 1bf1f4e2cefde5099452aec9dbc9386fea369777 127.0.0.1:8003
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
S: f72e454d7a592aef786ce318c4b73de6a0385a9d 127.0.0.1:8004
   slots: (0 slots) slave
   replicates 1bf1f4e2cefde5099452aec9dbc9386fea369777
M: 9dd8ecfd28eae160d8c19d4184c4777f9bde49e8 127.0.0.1:8005
   slots:[0-5460] (5461 slots) master
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 127.0.0.1:8006 to make it join the cluster.
Waiting for the cluster to join

>>> Configure node as replica of 127.0.0.1:8003.
[OK] New node added correctly.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

创建集群(客户端连接集群redis-cli需要带上-c,redis-cli -c -p port): 
1. redis-server 8001[8002|8003|8004|8005|8006|].conf,手动开启每个节点
2. redis-cli --cluster create 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006 --cluster-replicas 1
The option --cluster-replicas 1 means that we want a slave for every master created.(create a cluster with 3 masters and 3 slaves)
The other arguments are the list of addresses of the instances I want to use to create the new cluster.
集群中的主从是在创建集时分配,并不需要在每个节点的配置文件中设置

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

epoch(epoch is used in order to give incremental versioning to events)

The currentEpoch is a 64 bit unsigned number.At node creation every Redis Cluster node, both slaves and master nodes, set the currentEpoch to 0.
Every time a packet is received from another node, if the epoch of the sender (part of the cluster bus messages header) is greater than the local node epoch, the currentEpoch is updated to the sender epoch.
Because of these semantics, eventually all the nodes will agree to the greatest configEpoch in the cluster.
This information is used when the state of the cluster is changed and a node seeks agreement in order to perform some action.
Currently this happens only during slave promotion
In order to be elected, the first step for a slave is to increment its currentEpoch counter, and request votes from master instances.
Votes are requested by the slave by broadcasting a FAILOVER_AUTH_REQUEST packet to every master node of the cluster. 
Then it waits for a maximum time of two times the NODE_TIMEOUT for replies to arrive (but always for at least 2 seconds).
Once the slave receives ACKs from the majority of masters, it wins the election. Otherwise if the majority is not reached within the period of two times NODE_TIMEOUT (but always at least 2 seconds), 
the election is aborted and a new one will be tried again after NODE_TIMEOUT * 4 (and always at least 4 seconds).

The configEpoch is set to zero in masters when a new node is created.
the configEpoch helps to resolve conflicts when different nodes claim divergent configurations (a condition that may happen because of network partitions and node failures).
Slave nodes also advertise the configEpoch field in ping and pong packets, but in the case of slaves the field represents the configEpoch of its master as of the last time they exchanged packets. 
This allows other instances to detect when a slave has an old configuration that needs to be updated (master nodes will not grant votes to slaves with an old configuration).
Every time the configEpoch changes for some known node, it is permanently stored in the nodes.conf file by all the nodes that receive this information. The same also happens for the currentEpoch value. 
These two variables are guaranteed to be saved and fsync-ed to disk when updated before a node continues its operations.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Redis Cluster is not able to guarantee strong consistency. 
In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client.
The first reason why Redis Cluster can lose writes is because it uses asynchronous replication. This means that during writes the following happens:
Your client writes to the master B.
The master B replies OK to your client.
The master B propagates the write to its slaves B1, B2 and B3.
As you can see, B does not wait for an acknowledgement from B1, B2, B3 before replying to the client, since this would be a prohibitive latency penalty for Redis, 
so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

in practical terms, what do you get with Redis Cluster?
The ability to automatically split your dataset among multiple nodes.
The ability to continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster.

redis集群不支持事物
In redis-py-cluster, pipelining is all about trying to achieve greater network efficiency. 
Transaction support is disabled in redis-py-cluster. Use pipelines to avoid extra network round-trips, not to ensure atomicity.

Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot. 
The user can force multiple keys to be part of the same hash slot by using a concept called hash tags.
Hash tags are documented in the Redis Cluster specification, but the gist is that if there is a substring between {} brackets in a key, only what is inside the string is hashed, 
so for example this{foo}key and another{foo}key are guaranteed to be in the same hash slot, and can be used together in a command with multiple keys as arguments.
redis集群只支持db0,不支持mget,mset,multi,lua脚本,除非这些key落在同一个slot上,keys *只会返回该节点的数据(主从数据一样)
redis-py-cluster的StrictRedisCluster对keys,mget,mset,pipeline做了处理,使用时不需要再考虑key落在不同slot问题,但对于lua脚本则必须落在同一个slot上
其中pipeline原理是先根据for key in keys:crc16(key)%16384给keys分组,再批量执行,效率仍然比单条命令依次执行要高
参考:https://github.com/Grokzen/redis-py-cluster/blob/unstable/rediscluster/pipeline.py#L139 , send_cluster_commands

什么时候整个集群不可用(cluster_state:fail)?
如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot映射[0-16383]不完整时进入fail状态
如果集群超过半数以上master挂掉,无论是否有slave,集群进入fail状态

Redis集群中内置了16384(2的14次方)个哈希槽,当需要在Redis集群中放置一个key-value时,先对key使用crc16算法算出一个结果,然后把结果对16384求余数,
这样每个key都会对应一个编号在0-16383之间的哈希槽,redis会根据节点数量大致均等的将哈希槽映射到不同的节点(没使用一致性哈希)
投票过程是集群中所有master参与,如果半数以上master节点与master节点通信超时(cluster-node-timeout),认为当前master节点挂掉.
手动杀死集群中的某个master,其slave会自动被晋选为master

# an implementation of the HASH_SLOT function in python language. 
def hash_slot(key):
    left = key.find('{')   # 首次出现的位置
    if left>=0:
        right = key.find('}')
        if right-left>=2:  # 确保{}之间有字符
            return crc16(key[left+1:right]) & 0b11111111111111  # 16383
    return crc16(key) & 0b11111111111111  # 16383
